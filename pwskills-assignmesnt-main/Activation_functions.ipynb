{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6013785-91aa-4113-9958-8cfe6a9ef01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- Explain the role of activation functions in neural networks. Compare and contrast linear and nonlinear activation functions. Why are nonlinear activation functions preferred in hidden layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fb45a-3698-4188-ae54-9c1c66f172da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions are critical for introducing nonlinearity to neural networks, allowing them to learn complex patterns. Linear activation functions are limited to modeling linear relationships, while nonlinear activation functions, such as ReLU and Sigmoid, enable the network to handle more complex tasks. Nonlinear activation functions are preferred in hidden layers because they allow the network to capture intricate patterns in the data and improve learning and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8a3d2-1d86-4fb4-b8d5-673fa766eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2- Describe the Sigmoid activation function. What are its characteristics, and in what type of layers is it commonly used? Explain the Rectified Linear Unit (ReLU) activation function. Discuss its advantages and potential challenges.What is the purpose of the Tanh activation function? How does it differ from the Sigmoid activation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8b8a6-76e2-4ba9-b6b9-ed830709fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid: Outputs between 0 and 1, ideal for binary classification tasks, but suffers from vanishing gradients and non-zero-centered outputs.\n",
    "#ReLU: Outputs positive values and zero, commonly used in hidden layers for deep networks due to its efficiency and ability to avoid vanishing gradients. However, it can lead to \"dead\" neurons with the Dying ReLU problem.\n",
    "#Tanh: Outputs between -1 and 1 and is zero-centered, making it better for some hidden layers than Sigmoid, but it still faces the vanishing gradient issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c1258f-3dc6-4b72-9342-63f31a35c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3- Discuss the significance of activation functions in the hidden layers of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbd587-58bc-4aad-844f-4e3e8dd12af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions in hidden layers are fundamental to the success of neural networks, as they allow the network to model complex relationships, improve learning, and enable efficient gradient propagation. Without them, networks would not be able to solve anything beyond linear problems. The choice of activation function in hidden layers affects learning efficiency, model performance, and convergence speed, making it a key consideration in neural network design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58842d31-e676-409d-8130-d2e5c4a5665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4- Explain the choice of activation functions for different types of problems (e.g., classification, regression) in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8557af29-399c-4909-b059-c998fe88c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the correct activation function in the output layer is crucial because it directly affects how the model interprets its predictions. For classification tasks, Sigmoid and Softmax are used to output probabilities for discrete categories, while for regression tasks, a linear activation is used to predict continuous values. The activation function must align with the problem's nature to ensure correct model performance and interpretability of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165dbea-0478-4d32-b94c-1e0b929415c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- Experiment with different activation functions (e.g., ReLU, Sigmoid, Tanh) in a simple neural network architecture. Compare their effects on convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af239b1b-2210-4b13-9e88-9233f7eadddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6019ec-cf42-45e7-9284-a8653b1010d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with relu activation function:\n",
      "Test Accuracy: 0.9556, Test Loss: 0.1417\n",
      "\n",
      "Training model with sigmoid activation function:\n",
      "Test Accuracy: 0.8667, Test Loss: 0.3483\n",
      "\n",
      "Training model with tanh activation function:\n",
      "Test Accuracy: 0.9333, Test Loss: 0.2126\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to create and train the model with different activation functions\n",
    "def create_model(activation_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=4, activation=activation_function))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Activation functions to experiment with\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Train models with different activation functions and compare their performance\n",
    "for activation in activation_functions:\n",
    "    print(f\"\\nTraining model with {activation} activation function:\")\n",
    "    model = create_model(activation)\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)  # Training without verbose output\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, Test Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d5bc9c-b6a7-4757-90d8-20c5fdb551f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee1afa-59ea-48f0-afa4-c81f817b467c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777915fb-f8c7-47bb-82bf-3fc85625cfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf93df-ac65-4eac-94ce-f410b2d55432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfe352-ee7e-441e-b404-46f0d071750f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f2c90-1c1d-4a1d-bd78-f621ba622a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374ca35-169a-40cb-9708-d45df7aa5f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d5ae9-ae2f-49c5-b572-b137064fe5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a4368-3629-49db-adab-da01b43285f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d246781-5a94-405e-a8d2-9630d3a96787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9d966-3677-48a3-a3bd-727e7ee6e3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c041b-2c3b-4d29-9ec3-5a678577c79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ee0c1-c2b6-4ae5-87a8-b09920ec72d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be9811-3a2f-4cae-ac32-0b3acab79366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37675ee-3f35-4f70-987f-ff7fd6c95e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92215254-7c9f-417b-8cf1-198767adc85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0add9cc-614f-482e-981c-9e3930aefcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc6628-03da-4324-a423-bec0938203c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388bd9a-7c79-4780-b7ad-f30c35887deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b20e7-d246-4fd7-afca-fb5afd521266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9b1a6-f31d-4b4c-ba8f-17d1a5de97fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad1385-9331-45e6-88fa-f809255bf150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f01e84-6d55-465b-b1de-ca67b4452974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e48be-214f-4020-8946-a08faca1f621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5086d-20f2-49bf-bf05-707990dd6de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47715a5b-076d-4588-bc80-6590d9e6af42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1c236-9e8c-42bd-addf-7a8bf310a5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79e901-278c-4cbe-b5b5-5b9bed63d3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd9a22-6b72-481e-b2a7-d14b821f107f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18042796-0798-4106-ab93-94db4689cfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
